{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "983f79f6",
   "metadata": {},
   "source": [
    "# Queueboard data analysis notebook (pull requests)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16808389",
   "metadata": {},
   "source": [
    "## Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f57ae01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run me to download latest data from GitHub\n",
    "from download_artifact import download_and_extract_latest_successful_workflow_artifacts\n",
    "info = download_and_extract_latest_successful_workflow_artifacts(\n",
    "    repo=\"leanprover-community/queueboard-core\",\n",
    "    workflow=\"upload_backup.yaml\",\n",
    "    out_dir=\"./data\",\n",
    "    artifact_name=\"analytics-datasets\",\n",
    "    branch=\"master\",\n",
    "    search_limit=100,  # change this if you expect there to be > 100 failed runs before the first successful one\n",
    ")\n",
    "info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ed687c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load PR and PR event data\n",
    "from pathlib import Path\n",
    "import polars as pl\n",
    "from datetime import timezone\n",
    "\n",
    "data_dir = Path(\"data\")\n",
    "\n",
    "# Names of columns that should be converted to datetime\n",
    "DT_COLS = [\n",
    "    \"created_at\",\n",
    "    \"updated_at\",\n",
    "    \"gh_created_at\",\n",
    "    \"gh_updated_at\",\n",
    "    \"last_synced_at\",\n",
    "    \"commits_earliest_synced_at\",\n",
    "    # \"timeline_earliest_synced_at\", # TODO: why does this end up as Float64?\n",
    "    \"engagement_synced_at\",\n",
    "    \"occurred_at\",\n",
    "    \"closed_at\",\n",
    "    \"merged_at\",\n",
    "]\n",
    "\n",
    "def parse_dt_cols(df_raw):\n",
    "  return df_raw.with_columns([\n",
    "    pl.col(c)\n",
    "      # strangely, the %:::z format doesn't seem to work, but %#z seems to, even both are mentioned in the chrono crate docs\n",
    "      # https://docs.rs/chrono/latest/chrono/format/strftime/index.html\n",
    "      # (mentioned in polars docs https://docs.pola.rs/api/python/dev/reference/expressions/api/polars.Expr.dt.strftime.html)\n",
    "      .str.strptime(pl.Datetime('us', timezone.utc), format=\"%Y-%m-%d %T%.f%#z\", strict=False)\n",
    "      .alias(c)\n",
    "    for c in DT_COLS\n",
    "    if c in df_raw.columns\n",
    "  ])\n",
    "\n",
    "df_prs_raw = pl.read_parquet(data_dir / \"syncer_pullrequest.parquet\")\n",
    "df_prs = parse_dt_cols(df_prs_raw)\n",
    "\n",
    "df_events_raw = pl.read_parquet(data_dir / \"syncer_prtimelineevent.parquet\")\n",
    "df_events = parse_dt_cols(df_events_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191ed266",
   "metadata": {},
   "source": [
    "## Open intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ccb6ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "def build_pr_open_intervals(\n",
    "    df_prs: pl.DataFrame,\n",
    "    df_events: pl.DataFrame,\n",
    "    *,\n",
    "    asof: pl.Expr | None = None,\n",
    ") -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Build PR open intervals from PR created_at + CLOSED/REOPENED events.\n",
    "\n",
    "    Returns columns:\n",
    "      - pull_request_id\n",
    "      - start (Datetime)\n",
    "      - end (Datetime | null)\n",
    "      - is_open_ended (bool)\n",
    "      - end_effective (Datetime)\n",
    "      - duration (Duration)\n",
    "      - duration_hours (float)\n",
    "      - duration_days (float)\n",
    "    \"\"\"\n",
    "    if asof is None:\n",
    "        asof = pl.lit(datetime.now(tz=timezone.utc))\n",
    "\n",
    "    # --- PR creation change (+1) ---\n",
    "    prs_changes = (\n",
    "        df_prs\n",
    "        .select([\n",
    "            pl.col(\"id\").alias(\"pull_request_id\"),\n",
    "            pl.col(\"gh_created_at\").alias(\"occurred_at\"),\n",
    "        ])\n",
    "        .with_columns([\n",
    "            pl.lit(\"CREATED\").alias(\"etype\"),\n",
    "            pl.lit(1, dtype=pl.Int32).alias(\"change\"),\n",
    "            pl.lit(0, dtype=pl.Int32).alias(\"order\"),\n",
    "        ])\n",
    "        .drop_nulls([\"occurred_at\"])\n",
    "    )\n",
    "\n",
    "    # --- Event changes ---\n",
    "    ev_changes = (\n",
    "        df_events\n",
    "        .filter(pl.col(\"type\").is_in([\"CLOSED\", \"REOPENED\"]))\n",
    "        .select([\n",
    "            pl.col(\"pull_request_id\"),\n",
    "            pl.col(\"occurred_at\"),\n",
    "            pl.col(\"type\").alias(\"etype\"),\n",
    "        ])\n",
    "        .with_columns([\n",
    "            pl.when(pl.col(\"etype\") == \"CLOSED\").then(-1).otherwise(1).cast(pl.Int32).alias(\"change\"),\n",
    "            pl.when(pl.col(\"etype\") == \"REOPENED\").then(1).otherwise(2).cast(pl.Int32).alias(\"order\"),\n",
    "        ])\n",
    "        .drop_nulls([\"occurred_at\"])\n",
    "    )\n",
    "\n",
    "    changes = (\n",
    "        pl.concat([prs_changes, ev_changes], how=\"vertical\")\n",
    "        .sort([\"pull_request_id\", \"occurred_at\", \"order\"])\n",
    "        .with_columns([\n",
    "            pl.col(\"change\").cum_sum().over(\"pull_request_id\").alias(\"open_count\"),\n",
    "        ])\n",
    "        .with_columns([\n",
    "            pl.col(\"open_count\").shift(1).over(\"pull_request_id\").fill_null(0).alias(\"prev_open_count\"),\n",
    "        ])\n",
    "        .with_columns([\n",
    "            ((pl.col(\"prev_open_count\") <= 0) & (pl.col(\"open_count\") > 0)).alias(\"start_flag\"),\n",
    "            ((pl.col(\"prev_open_count\") > 0) & (pl.col(\"open_count\") <= 0)).alias(\"end_flag\"),\n",
    "        ])\n",
    "        .with_columns([\n",
    "            pl.col(\"start_flag\").cast(pl.Int32).cum_sum().over(\"pull_request_id\").alias(\"start_idx\"),\n",
    "            pl.col(\"end_flag\").cast(pl.Int32).cum_sum().over(\"pull_request_id\").alias(\"end_idx\"),\n",
    "        ])\n",
    "    )\n",
    "\n",
    "    starts = (\n",
    "        changes\n",
    "        .filter(pl.col(\"start_flag\"))\n",
    "        .select([\n",
    "            pl.col(\"pull_request_id\"),\n",
    "            pl.col(\"start_idx\").alias(\"interval_idx\"),\n",
    "            pl.col(\"occurred_at\").alias(\"start\"),\n",
    "        ])\n",
    "    )\n",
    "\n",
    "    ends = (\n",
    "        changes\n",
    "        .filter(pl.col(\"end_flag\"))\n",
    "        .select([\n",
    "            pl.col(\"pull_request_id\"),\n",
    "            pl.col(\"end_idx\").alias(\"interval_idx\"),\n",
    "            pl.col(\"occurred_at\").alias(\"end\"),\n",
    "        ])\n",
    "    )\n",
    "\n",
    "    intervals = (\n",
    "        starts\n",
    "        .join(ends, on=[\"pull_request_id\", \"interval_idx\"], how=\"left\")\n",
    "        .select([\"pull_request_id\", \"start\", \"end\"])\n",
    "        .sort([\"pull_request_id\", \"start\"])\n",
    "        .with_columns([\n",
    "            pl.col(\"end\").is_null().alias(\"is_open_ended\"),\n",
    "            pl.coalesce([pl.col(\"end\"), asof]).alias(\"end_effective\"),\n",
    "        ])\n",
    "        .with_columns([\n",
    "            (pl.col(\"end_effective\") - pl.col(\"start\")).alias(\"duration\"),\n",
    "        ])\n",
    "        .with_columns([\n",
    "            (pl.col(\"duration\").dt.total_seconds() / 3600.0).alias(\"duration_hours\"),\n",
    "            (pl.col(\"duration\").dt.total_seconds() / 86400.0).alias(\"duration_days\"),\n",
    "        ])\n",
    "    )\n",
    "\n",
    "    return intervals\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e03cc5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_intervals = build_pr_open_intervals(df_prs, df_events)\n",
    "\n",
    "# Or fix as-of time for reproducibility:\n",
    "# df_intervals_pl = build_pr_open_intervals(\n",
    "#     df_prs, df_events,\n",
    "#     asof=pl.lit(pl.datetime(2025, 12, 17, 0, 0, 0, time_unit=\"us\"))\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b16228",
   "metadata": {},
   "source": [
    "## Open PRs vs time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159ef8dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "def open_prs_per_day(\n",
    "    intervals: pl.DataFrame,\n",
    "    *,\n",
    "    start_col: str = \"start\",\n",
    "    end_effective_col: str = \"end_effective\",\n",
    "    asof: datetime | None = None,   # optional cutoff\n",
    ") -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Compute number of open PRs per day.\n",
    "\n",
    "    Counts a PR as open on a day if it is open at any point during that day.\n",
    "    For each interval [start, end_effective], we:\n",
    "      +1 on start_date\n",
    "      -1 on (end_date + 1 day)\n",
    "\n",
    "    Returns DataFrame with columns:\n",
    "      - date (Date)\n",
    "      - delta (i64)\n",
    "      - open_prs (i64)\n",
    "    \"\"\"\n",
    "\n",
    "    if asof is None:\n",
    "        asof = datetime.now(tz=timezone.utc)\n",
    "\n",
    "    asof_lit = pl.lit(asof)\n",
    "\n",
    "    df = (\n",
    "        intervals\n",
    "        .with_columns([\n",
    "            pl.col(start_col),\n",
    "            pl.col(end_effective_col),\n",
    "        ])\n",
    "        # cap end_effective at asof (optional but usually what you want)\n",
    "        .with_columns([\n",
    "            pl.when(pl.col(end_effective_col) > asof_lit)\n",
    "              .then(asof_lit)\n",
    "              .otherwise(pl.col(end_effective_col))\n",
    "              .alias(end_effective_col)\n",
    "        ])\n",
    "        .with_columns([\n",
    "            pl.col(start_col).dt.date().alias(\"start_date\"),\n",
    "            pl.col(end_effective_col).dt.date().alias(\"end_date\"),\n",
    "        ])\n",
    "    )\n",
    "\n",
    "    # +1 deltas on start_date\n",
    "    starts = (\n",
    "        df.group_by(\"start_date\")\n",
    "          .agg(pl.len().cast(pl.Int64).alias(\"delta\"))\n",
    "          .rename({\"start_date\": \"date\"})\n",
    "    )\n",
    "\n",
    "    # -1 deltas on day after end_date\n",
    "    ends = (\n",
    "        df.with_columns((pl.col(\"end_date\") + pl.duration(days=1)).alias(\"date\"))\n",
    "          .group_by(\"date\")\n",
    "          .agg((-pl.len().cast(pl.Int64)).alias(\"delta\"))\n",
    "    )\n",
    "\n",
    "    deltas = (\n",
    "        pl.concat([starts, ends], how=\"vertical\")\n",
    "          .group_by(\"date\")\n",
    "          .agg(pl.sum(\"delta\").alias(\"delta\"))\n",
    "          .sort(\"date\")\n",
    "    )\n",
    "\n",
    "    # Build a complete daily index and fill missing deltas with 0\n",
    "    min_date = df.select(pl.min(\"start_date\")).item()\n",
    "    max_date = df.select(pl.max(\"end_date\")).item()\n",
    "    all_days = pl.DataFrame({\n",
    "        \"date\": pl.date_range(min_date, max_date, interval=\"1d\", eager=True)\n",
    "    })\n",
    "\n",
    "    out = (\n",
    "        all_days\n",
    "        .join(deltas, on=\"date\", how=\"left\")\n",
    "        .with_columns(pl.col(\"delta\").fill_null(0))\n",
    "        .sort(\"date\")\n",
    "        .with_columns(pl.col(\"delta\").cum_sum().alias(\"open_prs\"))\n",
    "    )\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec63779",
   "metadata": {},
   "outputs": [],
   "source": [
    "import altair\n",
    "daily = open_prs_per_day(df_intervals)\n",
    "daily.plot.line(x=\"date\", y=\"open_prs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "584ce33d",
   "metadata": {},
   "source": [
    "## Interval plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b30e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "from datetime import datetime, timezone\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.collections import LineCollection\n",
    "import matplotlib.dates as mdates\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def prepare_swimlane_polars(\n",
    "    intervals: pl.DataFrame,\n",
    "    *,\n",
    "    asof: datetime | None = None,\n",
    "    max_prs: int | None = None,   # optional downsample: keep earliest max_prs PRs\n",
    ") -> pl.DataFrame:\n",
    "    if asof is None:\n",
    "        asof = datetime.now(tz=timezone.utc)\n",
    "\n",
    "    # ensure end_effective exists\n",
    "    df = intervals.with_columns(\n",
    "        pl.coalesce([pl.col(\"end\"), pl.lit(asof)]).alias(\"end_effective\")\n",
    "    )\n",
    "\n",
    "    # first open per PR -> defines order\n",
    "    order = (\n",
    "        df.group_by(\"pull_request_id\")\n",
    "          .agg(pl.min(\"start\").alias(\"first_open\"))\n",
    "          .sort(\"first_open\")\n",
    "          .with_row_index(\"y\")              # y = 0..n-1 in sorted order\n",
    "    )\n",
    "\n",
    "    if max_prs is not None:\n",
    "        order = order.head(max_prs)\n",
    "\n",
    "    # attach y back to each interval\n",
    "    out = (\n",
    "        df.join(order.select([\"pull_request_id\", \"y\"]), on=\"pull_request_id\", how=\"inner\")\n",
    "          .select([\"pull_request_id\", \"start\", \"end_effective\", \"y\"])\n",
    "          .sort([\"y\", \"start\"])\n",
    "    )\n",
    "    return out\n",
    "\n",
    "def plot_swimlane_matplotlib(swim: pl.DataFrame, *, figsize=(12, 12), linewidth=0.5, title=None):\n",
    "    start = swim[\"start\"].to_numpy()\n",
    "    end   = swim[\"end_effective\"].to_numpy()\n",
    "    y     = swim[\"y\"].to_numpy()\n",
    "\n",
    "    x0 = mdates.date2num(start)\n",
    "    x1 = mdates.date2num(end)\n",
    "\n",
    "    segments = [((a, yi), (b, yi)) for a, b, yi in zip(x0, x1, y)]\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    lc = LineCollection(segments, linewidths=linewidth)\n",
    "    lc.set_rasterized(True)\n",
    "    ax.add_collection(lc)\n",
    "\n",
    "    ax.set_xlim(x0.min(), x1.max())\n",
    "    ax.set_ylim(-1, y.max() + 1)\n",
    "    ax.xaxis_date()\n",
    "    fig.autofmt_xdate()\n",
    "\n",
    "    ax.set_xlabel(\"Time\")\n",
    "    ax.set_ylabel(\"PR (earliest at bottom)\")\n",
    "    ax.set_title(title or \"PR Open Intervals (swimlane)\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9df11eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "swim_all = prepare_swimlane_polars(df_intervals)   # all PRs\n",
    "plot_swimlane_matplotlib(swim_all, figsize=(5, 25), linewidth=0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca8394b",
   "metadata": {},
   "source": [
    "## Filtering Classes of Intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc9640bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "\n",
    "def enrich_intervals_with_prs(\n",
    "    intervals: pl.DataFrame,\n",
    "    prs: pl.DataFrame,\n",
    "    *,\n",
    "    pr_id_col: str = \"pull_request_id\",\n",
    "    prs_id_col: str = \"id\",\n",
    "    pr_cols: list[str] | None = None,\n",
    ") -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Left-join PR metadata onto interval rows.\n",
    "\n",
    "    intervals: must have pull_request_id\n",
    "    prs: must have id and any columns you want to join\n",
    "    pr_cols: PR columns to bring over (excluding id). If None, uses a sensible default.\n",
    "    \"\"\"\n",
    "    if pr_cols is None:\n",
    "        pr_cols = [\n",
    "            \"number\", \"state\", \"is_draft\",\n",
    "            \"repository_id\", \"author_id\",\n",
    "            \"base_ref_name\", \"head_ref_name\",\n",
    "            \"title\",\n",
    "            \"additions\", \"deletions\", \"changed_files_count\",\n",
    "            \"gh_created_at\", \"closed_at\", \"merged_at\",\n",
    "        ]\n",
    "\n",
    "    cols_present = [c for c in pr_cols if c in prs.columns]\n",
    "    prs_small = prs.select([pl.col(prs_id_col).alias(pr_id_col), *[pl.col(c) for c in cols_present]])\n",
    "\n",
    "    return intervals.join(prs_small, on=pr_id_col, how=\"left\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a39105a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_intervals_enriched = enrich_intervals_with_prs(df_intervals, df_prs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4d2734",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# title related filters\n",
    "\n",
    "def expr_title_contains(substr: str, *, case_insensitive: bool = True) -> pl.Expr:\n",
    "    # literal substring match (fast, safe)\n",
    "    return pl.col(\"title\").fill_null(\"\").str.to_lowercase().str.contains(\n",
    "        substr.lower(), literal=True\n",
    "    ) if case_insensitive else pl.col(\"title\").fill_null(\"\").str.contains(substr, literal=True)\n",
    "\n",
    "def expr_title_any(substrs: list[str], *, case_insensitive: bool = True) -> pl.Expr:\n",
    "    e = pl.lit(False)\n",
    "    for s in substrs:\n",
    "        e = e | expr_title_contains(s, case_insensitive=case_insensitive)\n",
    "    return e\n",
    "\n",
    "def expr_title_regex(pattern: str, *, case_insensitive: bool = True) -> pl.Expr:\n",
    "    # regex match\n",
    "    if case_insensitive:\n",
    "        # prefix with (?i) for case-insensitive\n",
    "        pattern = \"(?i)\" + pattern\n",
    "    return pl.col(\"title\").fill_null(\"\").str.contains(pattern, literal=False)\n",
    "\n",
    "def expr_title_exclude_any(substrs: list[str], *, case_insensitive: bool = True) -> pl.Expr:\n",
    "    return ~expr_title_any(substrs, case_insensitive=case_insensitive)\n",
    "\n",
    "# PR property filters\n",
    "\n",
    "def expr_repo_in(repo_ids: list[int]) -> pl.Expr:\n",
    "    return pl.col(\"repository_id\").is_in(repo_ids)\n",
    "\n",
    "def expr_author_in(author_ids: list[int]) -> pl.Expr:\n",
    "    return pl.col(\"author_id\").is_in(author_ids)\n",
    "\n",
    "def expr_base_branch_in(branches: list[str]) -> pl.Expr:\n",
    "    return pl.col(\"base_ref_name\").is_in(branches)\n",
    "\n",
    "def expr_state_is(state: str) -> pl.Expr:\n",
    "    return pl.col(\"state\") == state\n",
    "\n",
    "def expr_is_draft(is_draft: bool = True) -> pl.Expr:\n",
    "    return pl.col(\"is_draft\") == is_draft\n",
    "\n",
    "# size / churn filters\n",
    "\n",
    "def expr_additions_between(lo: int | None = None, hi: int | None = None) -> pl.Expr:\n",
    "    e = pl.lit(True)\n",
    "    if lo is not None:\n",
    "        e = e & (pl.col(\"additions\").fill_null(0) >= lo)\n",
    "    if hi is not None:\n",
    "        e = e & (pl.col(\"additions\").fill_null(0) <= hi)\n",
    "    return e\n",
    "\n",
    "def expr_churn_between(lo: int | None = None, hi: int | None = None) -> pl.Expr:\n",
    "    churn = pl.col(\"additions\").fill_null(0) + pl.col(\"deletions\").fill_null(0)\n",
    "    e = pl.lit(True)\n",
    "    if lo is not None:\n",
    "        e = e & (churn >= lo)\n",
    "    if hi is not None:\n",
    "        e = e & (churn <= hi)\n",
    "    return e\n",
    "\n",
    "# interval / date filters\n",
    "\n",
    "from datetime import datetime, timezone\n",
    "from typing import Union\n",
    "import pandas as pd\n",
    "\n",
    "DateLike = Union[str, datetime]\n",
    "\n",
    "def _to_utc_datetime(x: DateLike | None) -> datetime | None:\n",
    "    if x is None:\n",
    "        return None\n",
    "\n",
    "    if isinstance(x, datetime):\n",
    "        # ensure tz-aware UTC\n",
    "        if x.tzinfo is None:\n",
    "            return x.replace(tzinfo=timezone.utc)\n",
    "        return x.astimezone(timezone.utc)\n",
    "\n",
    "    if isinstance(x, str):\n",
    "        try:\n",
    "            dt = pd.to_datetime(x, utc=True)\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Could not parse datetime string: {x!r}\") from e\n",
    "        return dt.to_pydatetime()\n",
    "\n",
    "    raise TypeError(f\"Expected datetime or str, got {type(x)}\")\n",
    "\n",
    "def expr_interval_started_between(\n",
    "    *,\n",
    "    start_after: DateLike | None = None,\n",
    "    start_before: DateLike | None = None,\n",
    ") -> pl.Expr:\n",
    "    start_after_dt = _to_utc_datetime(start_after)\n",
    "    start_before_dt = _to_utc_datetime(start_before)\n",
    "\n",
    "    e = pl.lit(True)\n",
    "\n",
    "    if start_after_dt is not None:\n",
    "        e = e & (pl.col(\"start\") >= pl.lit(start_after_dt))\n",
    "\n",
    "    if start_before_dt is not None:\n",
    "        e = e & (pl.col(\"start\") < pl.lit(start_before_dt))\n",
    "\n",
    "    return e\n",
    "\n",
    "def expr_only_closed(only_closed: bool = True) -> pl.Expr:\n",
    "    return pl.col(\"is_open_ended\") == (not only_closed)\n",
    "\n",
    "# wrapper for filters\n",
    "\n",
    "def filter_intervals(df: pl.DataFrame, *exprs: pl.Expr) -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Apply multiple Polars boolean expressions as filters.\n",
    "    \"\"\"\n",
    "    if not exprs:\n",
    "        return df\n",
    "    e = pl.lit(True)\n",
    "    for ex in exprs:\n",
    "        e = e & ex\n",
    "    return df.filter(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a432af80",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_closed = filter_intervals(df_intervals_enriched, expr_only_closed(True))\n",
    "print('df_closed', len(df_closed))\n",
    "\n",
    "df_open = filter_intervals(df_intervals_enriched, expr_only_closed(False))\n",
    "print('df_open', len(df_open))\n",
    "df_open_feat = filter_intervals(df_open, expr_title_regex(r\"^feat\"))\n",
    "print('df_open_feat', len(df_open_feat))\n",
    "\n",
    "# doesn't count PRs that actually got merged\n",
    "df_merged = filter_intervals(df_closed, expr_title_regex(r\"^\\[Merged by Bors\\] -\"))\n",
    "print('df_merged', len(df_merged))\n",
    "df_merged_feat = filter_intervals(df_closed, expr_title_regex(r\"^\\[Merged by Bors\\] -\\s+[fF]eat\"))\n",
    "print('df_merged_feat', len(df_merged_feat))\n",
    "df_merged_nonfeat = filter_intervals(df_closed, expr_title_regex(r\"^\\[Merged by Bors\\] -\\s+[^\\sfF]\"))\n",
    "print('df_merged_nonfeat', len(df_merged_nonfeat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "011ef9e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged_2021 = filter_intervals(df_merged, expr_interval_started_between(start_after='2021-01-01', start_before='2022-01-01'))\n",
    "print('df_merged_2021', len(df_merged_2021))\n",
    "df_merged_2022 = filter_intervals(df_merged, expr_interval_started_between(start_after='2022-01-01', start_before='2023-01-01'))\n",
    "print('df_merged_2022', len(df_merged_2022))\n",
    "df_merged_2023 = filter_intervals(df_merged, expr_interval_started_between(start_after='2023-01-01', start_before='2024-01-01'))\n",
    "print('df_merged_2023', len(df_merged_2023))\n",
    "df_merged_2024 = filter_intervals(df_merged, expr_interval_started_between(start_after='2024-01-01', start_before='2025-01-01'))\n",
    "print('df_merged_2024', len(df_merged_2024))\n",
    "df_merged_2025 = filter_intervals(df_merged, expr_interval_started_between(start_after='2025-01-01', start_before='2026-01-01'))\n",
    "print('df_merged_2025', len(df_merged_2025))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c5898d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged_feat_after_port = filter_intervals(df_merged_feat, expr_interval_started_between(start_after='2023-08-01'))\n",
    "print('df_merged_feat_after_port', len(df_merged_feat_after_port))\n",
    "df_merged_nonfeat_after_port = filter_intervals(df_merged_nonfeat, expr_interval_started_between(start_after='2023-08-01'))\n",
    "print('df_merged_nonfeat_after_port', len(df_merged_nonfeat_after_port))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f7723fd",
   "metadata": {},
   "source": [
    "## Open Durations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c8bdc2d",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a002fa77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import polars as pl\n",
    "\n",
    "def get_x(df: pl.DataFrame, col: str) -> np.ndarray:\n",
    "    x = df.select(pl.col(col)).to_numpy().ravel().astype(float)\n",
    "    x = x[np.isfinite(x) & (x > 0)]\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d001fd22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import lognorm, weibull_min, fisk\n",
    "\n",
    "def plot_duration_hist(\n",
    "    df: pl.DataFrame,\n",
    "    *,\n",
    "    col=\"duration_days\",\n",
    "    bins=100,\n",
    "    logx=False,\n",
    "    logy=False,\n",
    "    exponential_fit=False,\n",
    "    title=None,\n",
    "):\n",
    "    x = get_x(df, col)\n",
    "    if x.size < 2:\n",
    "        print(\"Not enough data to plot.\")\n",
    "        return\n",
    "\n",
    "    # bins\n",
    "    if logx:\n",
    "        lo = max(x.min(), np.nextafter(0, 1))\n",
    "        hi = x.max()\n",
    "        bin_edges = np.logspace(np.log10(lo), np.log10(hi), bins + 1)\n",
    "    else:\n",
    "        bin_edges = np.linspace(x.min(), x.max(), bins + 1)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 4))\n",
    "    counts, edges, _ = ax.hist(x, bins=bin_edges)\n",
    "\n",
    "    if logx:\n",
    "        ax.set_xscale(\"log\")\n",
    "    if logy:\n",
    "        ax.set_yscale(\"log\")\n",
    "\n",
    "    if exponential_fit and logy:\n",
    "        N = x.size\n",
    "        lam = 1.0 / x.mean()\n",
    "        centers = 0.5 * (edges[:-1] + edges[1:])\n",
    "        widths = np.diff(edges)\n",
    "        expected = N * lam * np.exp(-lam * centers) * widths\n",
    "        ax.plot(centers, expected, linewidth=3, label=f\"Exponential (λ={lam:.3g})\")\n",
    "        ax.legend()\n",
    "\n",
    "    ax.set_xlabel(col.replace(\"_\", \" \"))\n",
    "    ax.set_ylabel(\"Count\")\n",
    "    ax.set_title(title or f\"Histogram of {col}\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_lognormal_fit_counts_logbins(df, *, col=\"duration_days\", bins=100, title=None):\n",
    "    x = get_x(df, col)\n",
    "    N = x.size\n",
    "    if N < 2:\n",
    "        print(\"Not enough data.\")\n",
    "        return None\n",
    "\n",
    "    lo = max(x.min(), np.nextafter(0, 1))\n",
    "    hi = x.max()\n",
    "    edges = np.logspace(np.log10(lo), np.log10(hi), bins + 1)\n",
    "    centers = np.sqrt(edges[:-1] * edges[1:])\n",
    "\n",
    "    sigma, loc, scale = lognorm.fit(x, floc=0)\n",
    "    mu = np.log(scale)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 4))\n",
    "    ax.hist(x, bins=edges, density=False, alpha=0.6)\n",
    "\n",
    "    cdf = lognorm.cdf(edges, s=sigma, loc=loc, scale=scale)\n",
    "    expected = N * np.diff(cdf)\n",
    "    ax.plot(centers, expected, linewidth=3, label=f\"Lognormal (μ={mu:.2f}, σ={sigma:.2f})\")\n",
    "\n",
    "    ax.set_xscale(\"log\")\n",
    "    ax.set_xlabel(col.replace(\"_\", \" \"))\n",
    "    ax.set_ylabel(\"Count per log bin\")\n",
    "    ax.set_title(title or \"Lognormal fit (counts)\")\n",
    "    ax.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return {\"mu\": float(mu), \"sigma\": float(sigma), \"n\": int(N)}\n",
    "\n",
    "def plot_weibull_fit_counts_logbins(df, *, col=\"duration_days\", bins=100, title=None):\n",
    "    x = get_x(df, col)\n",
    "    N = x.size\n",
    "    if N < 2:\n",
    "        print(\"Not enough data.\")\n",
    "        return None\n",
    "\n",
    "    lo = max(x.min(), np.nextafter(0, 1))\n",
    "    hi = x.max()\n",
    "    edges = np.logspace(np.log10(lo), np.log10(hi), bins + 1)\n",
    "    centers = np.sqrt(edges[:-1] * edges[1:])\n",
    "\n",
    "    k, loc, scale = weibull_min.fit(x, floc=0)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 4))\n",
    "    ax.hist(x, bins=edges, density=False, alpha=0.6)\n",
    "\n",
    "    cdf = weibull_min.cdf(edges, k, loc=loc, scale=scale)\n",
    "    expected = N * np.diff(cdf)\n",
    "    ax.plot(centers, expected, linewidth=3, label=f\"Weibull (k={k:.2f})\")\n",
    "\n",
    "    ax.set_xscale(\"log\")\n",
    "    ax.set_xlabel(col.replace(\"_\", \" \"))\n",
    "    ax.set_ylabel(\"Count per log bin\")\n",
    "    ax.set_title(title or \"Weibull fit (counts)\")\n",
    "    ax.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return {\"k\": float(k), \"scale\": float(scale), \"n\": int(N)}\n",
    "\n",
    "def plot_loglogistic_fit_counts_logbins(df, *, col=\"duration_days\", bins=100, title=None):\n",
    "    x = get_x(df, col)\n",
    "    N = x.size\n",
    "    if N < 2:\n",
    "        print(\"Not enough data.\")\n",
    "        return None\n",
    "\n",
    "    lo = max(x.min(), np.nextafter(0, 1))\n",
    "    hi = x.max()\n",
    "    edges = np.logspace(np.log10(lo), np.log10(hi), bins + 1)\n",
    "    centers = np.sqrt(edges[:-1] * edges[1:])\n",
    "\n",
    "    c, loc, scale = fisk.fit(x, floc=0)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 4))\n",
    "    ax.hist(x, bins=edges, density=False, alpha=0.6)\n",
    "\n",
    "    cdf = fisk.cdf(edges, c, loc=loc, scale=scale)\n",
    "    expected = N * np.diff(cdf)\n",
    "    ax.plot(centers, expected, linewidth=3, label=f\"Log-logistic (c={c:.2f})\")\n",
    "\n",
    "    ax.set_xscale(\"log\")\n",
    "    ax.set_xlabel(col.replace(\"_\", \" \"))\n",
    "    ax.set_ylabel(\"Count per log bin\")\n",
    "    ax.set_title(title or \"Log-logistic fit (counts)\")\n",
    "    ax.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return {\"c\": float(c), \"scale\": float(scale), \"n\": int(N)}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11df198",
   "metadata": {},
   "source": [
    "### Basic plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d01826d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_duration_hist(df_closed, logx=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1df8eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_duration_hist(df_open, logx=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95d1c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_duration_hist(df_open, logy=True, exponential_fit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e823e8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "1/0.00417"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7211d470",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_open['duration_days'].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ec71cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_duration_hist(df_merged, logx=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa3ee49",
   "metadata": {},
   "source": [
    "### Plots with fits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19bedbd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_lognormal_params = plot_lognormal_fit_counts_logbins(df_merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8081f260",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.exp(merged_lognormal_params['mu'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af30d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged['duration_days'].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a5976c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loglogistic_fit_counts_logbins(df_merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a7d7e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_weibull_fit_counts_logbins(df_merged)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab9812b",
   "metadata": {},
   "source": [
    "### More lognormal plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa76dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_lognormal_fit_counts_logbins(df_merged, col=\"duration_days\", bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1330923",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.exp(0.454)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad420956",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged['duration_days'].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42eea6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_lognormal_fit_counts_logbins(df_merged_2021, col=\"duration_days\", bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17089397",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_lognormal_fit_counts_logbins(df_merged_2022, col=\"duration_days\", bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d186d2cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_lognormal_fit_counts_logbins(df_merged_2023, col=\"duration_days\", bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d51d60ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_lognormal_fit_counts_logbins(df_merged_2024, col=\"duration_days\", bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8902ac0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_lognormal_fit_counts_logbins(df_merged_2025, col=\"duration_days\", bins=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90cb3dbe",
   "metadata": {},
   "source": [
    "### Combined plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb98403b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import lognorm\n",
    "\n",
    "def plot_hist_and_lognormal_fit_overlays(\n",
    "    dfs: list[pl.DataFrame],\n",
    "    labels: list[str],\n",
    "    *,\n",
    "    col: str = \"duration_days\",\n",
    "    bins: int = 100,\n",
    "    title: str | None = None,\n",
    "    show_params_in_legend: bool = False,\n",
    "    hist_linewidth: float = 1.2,\n",
    "    fit_linewidth: float = 3.0,\n",
    "):\n",
    "    if len(dfs) != len(labels):\n",
    "        raise ValueError(\"dfs and labels must have the same length.\")\n",
    "\n",
    "    xs = []\n",
    "    for df in dfs:\n",
    "        x = get_x(df, col)\n",
    "        xs.append(x)\n",
    "\n",
    "    allx = np.concatenate([x for x in xs if x.size])\n",
    "    if allx.size < 2:\n",
    "        raise ValueError(\"Not enough data across datasets to plot.\")\n",
    "\n",
    "    lo = max(allx.min(), np.nextafter(0, 1))\n",
    "    hi = allx.max()\n",
    "    edges = np.logspace(np.log10(lo), np.log10(hi), bins + 1)\n",
    "    centers = np.sqrt(edges[:-1] * edges[1:])\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 4))\n",
    "\n",
    "    for x, label in zip(xs, labels):\n",
    "        if x.size < 2:\n",
    "            continue\n",
    "\n",
    "        # Histogram counts (shared edges)\n",
    "        hist_counts, _ = np.histogram(x, bins=edges)\n",
    "\n",
    "        # Step outline histogram\n",
    "        y_step = np.r_[hist_counts, hist_counts[-1]]\n",
    "        (hist_line,) = ax.step(edges, y_step, where=\"post\", linewidth=hist_linewidth, alpha=0.9)\n",
    "        color = hist_line.get_color()\n",
    "\n",
    "        # Lognormal fit (loc=0) -> expected counts per bin\n",
    "        sigma, loc, scale = lognorm.fit(x, floc=0)\n",
    "        mu = np.log(scale)\n",
    "        cdf = lognorm.cdf(edges, s=sigma, loc=loc, scale=scale)\n",
    "        expected = x.size * np.diff(cdf)\n",
    "\n",
    "        fit_label = label\n",
    "        if show_params_in_legend:\n",
    "            fit_label = f\"{label} (μ={mu:.2f}, σ={sigma:.2f}, n={x.size})\"\n",
    "\n",
    "        ax.plot(centers, expected, color=color, linewidth=fit_linewidth, label=fit_label)\n",
    "\n",
    "    ax.set_xscale(\"log\")\n",
    "    ax.set_xlabel(col.replace(\"_\", \" \"))\n",
    "    ax.set_ylabel(\"Count per log bin\")\n",
    "    ax.set_title(title or \"Overlaid histograms + lognormal fits\")\n",
    "    ax.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f91928a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dfs = [df_merged_2021, df_merged_2022, df_merged_2023, df_merged_2024, df_merged_2025]\n",
    "labels = [\"2021\", \"2022\", \"2023\", \"2024\", \"2025\"]\n",
    "\n",
    "plot_hist_and_lognormal_fit_overlays(\n",
    "    dfs,\n",
    "    labels,\n",
    "    col=\"duration_days\",\n",
    "    bins=100,\n",
    "    title=\"Merged PR interval durations\",\n",
    "    show_params_in_legend=False,  # set True if you want μ/σ in legend\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65aa6fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df_merged_2021), len(df_merged_2022), len(df_merged_2023), len(df_merged_2024), len(df_merged_2025))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378d0657",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_feat = [df_merged_feat, df_merged_nonfeat]\n",
    "labels = [\"feat\", \"non-feat\"]\n",
    "\n",
    "plot_hist_and_lognormal_fit_overlays(\n",
    "    dfs_feat,\n",
    "    labels,\n",
    "    col=\"duration_days\",\n",
    "    bins=100,\n",
    "    title=\"Merged PR interval durations\",\n",
    "    show_params_in_legend=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16bf0cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.exp(1.14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2959628d",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.exp(-0.32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ef96c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_feat_after_port = [df_merged_feat_after_port, df_merged_nonfeat_after_port]\n",
    "labels = [\"feat (after 2023-08-01)\", \"non-feat (after 2023-08-01)\"]\n",
    "\n",
    "plot_hist_and_lognormal_fit_overlays(\n",
    "    dfs_feat_after_port,\n",
    "    labels,\n",
    "    col=\"duration_days\",\n",
    "    bins=100,\n",
    "    title=\"Merged PR interval durations\",\n",
    "    show_params_in_legend=True,  # set True if you want μ/σ in legend\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d99ebaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.exp(1.57)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b66253",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.exp(-0.22)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34fcae6",
   "metadata": {},
   "source": [
    "## TODO: similar plots for review cycles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55205476",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qb-notebook",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
